{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrival and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'TATAMOTORS.NS'\n",
    "num_of_days = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n",
      "WARNING:tensorflow:From /Users/sabrish/Documents/Training/machine-learning-master/capstone/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from models.MLPModel import MLPModel\n",
    "\n",
    "model = MLPModel(ticker, num_of_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-01-01</th>\n",
       "      <td>0.657293</td>\n",
       "      <td>0.691786</td>\n",
       "      <td>0.655377</td>\n",
       "      <td>0.690637</td>\n",
       "      <td>0.156121</td>\n",
       "      <td>116802.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-02</th>\n",
       "      <td>0.695619</td>\n",
       "      <td>0.695619</td>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.683642</td>\n",
       "      <td>0.154540</td>\n",
       "      <td>128507.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-03</th>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.689774</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.155320</td>\n",
       "      <td>79989.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-06</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.677414</td>\n",
       "      <td>0.153132</td>\n",
       "      <td>56131.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-07</th>\n",
       "      <td>0.675977</td>\n",
       "      <td>0.675977</td>\n",
       "      <td>0.653939</td>\n",
       "      <td>0.659113</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>114660.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-08</th>\n",
       "      <td>0.669749</td>\n",
       "      <td>0.684121</td>\n",
       "      <td>0.665437</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.151464</td>\n",
       "      <td>52462.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-09</th>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.675498</td>\n",
       "      <td>0.663042</td>\n",
       "      <td>0.667928</td>\n",
       "      <td>0.150987</td>\n",
       "      <td>56314.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-10</th>\n",
       "      <td>0.674540</td>\n",
       "      <td>0.689678</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.685175</td>\n",
       "      <td>0.154886</td>\n",
       "      <td>97257.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-13</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.669366</td>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.152915</td>\n",
       "      <td>42305.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-14</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.683546</td>\n",
       "      <td>0.657389</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.150099</td>\n",
       "      <td>65088.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-15</th>\n",
       "      <td>0.670803</td>\n",
       "      <td>0.710566</td>\n",
       "      <td>0.670707</td>\n",
       "      <td>0.710375</td>\n",
       "      <td>0.160583</td>\n",
       "      <td>84981.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-16</th>\n",
       "      <td>0.720531</td>\n",
       "      <td>0.758665</td>\n",
       "      <td>0.666874</td>\n",
       "      <td>0.677510</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>113797.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-17</th>\n",
       "      <td>0.661796</td>\n",
       "      <td>0.682109</td>\n",
       "      <td>0.643017</td>\n",
       "      <td>0.662179</td>\n",
       "      <td>0.149688</td>\n",
       "      <td>103796.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-20</th>\n",
       "      <td>0.661126</td>\n",
       "      <td>0.662850</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>0.641483</td>\n",
       "      <td>0.145009</td>\n",
       "      <td>51001.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-21</th>\n",
       "      <td>0.637172</td>\n",
       "      <td>0.668024</td>\n",
       "      <td>0.632956</td>\n",
       "      <td>0.641771</td>\n",
       "      <td>0.145075</td>\n",
       "      <td>81317.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-22</th>\n",
       "      <td>0.653173</td>\n",
       "      <td>0.662084</td>\n",
       "      <td>0.632860</td>\n",
       "      <td>0.639759</td>\n",
       "      <td>0.144620</td>\n",
       "      <td>81056.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-24</th>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.626153</td>\n",
       "      <td>0.641771</td>\n",
       "      <td>0.145075</td>\n",
       "      <td>93348.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-27</th>\n",
       "      <td>0.686708</td>\n",
       "      <td>0.686708</td>\n",
       "      <td>0.647903</td>\n",
       "      <td>0.677989</td>\n",
       "      <td>0.153262</td>\n",
       "      <td>131776.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-28</th>\n",
       "      <td>0.683930</td>\n",
       "      <td>0.689870</td>\n",
       "      <td>0.662179</td>\n",
       "      <td>0.676360</td>\n",
       "      <td>0.152894</td>\n",
       "      <td>84234.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-29</th>\n",
       "      <td>0.687954</td>\n",
       "      <td>0.698398</td>\n",
       "      <td>0.654418</td>\n",
       "      <td>0.664671</td>\n",
       "      <td>0.150251</td>\n",
       "      <td>66964.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-30</th>\n",
       "      <td>0.661126</td>\n",
       "      <td>0.662563</td>\n",
       "      <td>0.638226</td>\n",
       "      <td>0.641196</td>\n",
       "      <td>0.144945</td>\n",
       "      <td>65678.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-31</th>\n",
       "      <td>0.628548</td>\n",
       "      <td>0.632381</td>\n",
       "      <td>0.609385</td>\n",
       "      <td>0.615038</td>\n",
       "      <td>0.139031</td>\n",
       "      <td>58224.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-03</th>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.612451</td>\n",
       "      <td>0.584569</td>\n",
       "      <td>0.589743</td>\n",
       "      <td>0.133313</td>\n",
       "      <td>55484.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-04</th>\n",
       "      <td>0.586390</td>\n",
       "      <td>0.599804</td>\n",
       "      <td>0.580832</td>\n",
       "      <td>0.589360</td>\n",
       "      <td>0.133227</td>\n",
       "      <td>74941.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-05</th>\n",
       "      <td>0.605457</td>\n",
       "      <td>0.616763</td>\n",
       "      <td>0.597887</td>\n",
       "      <td>0.599516</td>\n",
       "      <td>0.135523</td>\n",
       "      <td>38882.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-06</th>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.613026</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.608523</td>\n",
       "      <td>0.137559</td>\n",
       "      <td>26402.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-07</th>\n",
       "      <td>0.611206</td>\n",
       "      <td>0.624716</td>\n",
       "      <td>0.601720</td>\n",
       "      <td>0.612739</td>\n",
       "      <td>0.138512</td>\n",
       "      <td>35743.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-10</th>\n",
       "      <td>0.615134</td>\n",
       "      <td>0.615709</td>\n",
       "      <td>0.606128</td>\n",
       "      <td>0.610822</td>\n",
       "      <td>0.138078</td>\n",
       "      <td>20894.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-11</th>\n",
       "      <td>0.612260</td>\n",
       "      <td>0.640909</td>\n",
       "      <td>0.608427</td>\n",
       "      <td>0.635447</td>\n",
       "      <td>0.143645</td>\n",
       "      <td>38670.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-12</th>\n",
       "      <td>0.631327</td>\n",
       "      <td>0.649628</td>\n",
       "      <td>0.628644</td>\n",
       "      <td>0.638321</td>\n",
       "      <td>0.144295</td>\n",
       "      <td>28267.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>4.194500</td>\n",
       "      <td>4.234500</td>\n",
       "      <td>4.170000</td>\n",
       "      <td>4.218000</td>\n",
       "      <td>4.218000</td>\n",
       "      <td>61914.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>4.210000</td>\n",
       "      <td>4.257000</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>40511.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>4.244500</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>67454.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>4.254500</td>\n",
       "      <td>4.330000</td>\n",
       "      <td>4.245000</td>\n",
       "      <td>4.286000</td>\n",
       "      <td>4.286000</td>\n",
       "      <td>78994.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>4.286000</td>\n",
       "      <td>4.310500</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>4.262500</td>\n",
       "      <td>4.262500</td>\n",
       "      <td>73742.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>4.270000</td>\n",
       "      <td>4.299500</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>51081.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>4.230000</td>\n",
       "      <td>4.247500</td>\n",
       "      <td>4.178000</td>\n",
       "      <td>4.214000</td>\n",
       "      <td>4.214000</td>\n",
       "      <td>79460.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>4.198000</td>\n",
       "      <td>4.222500</td>\n",
       "      <td>4.145000</td>\n",
       "      <td>4.159500</td>\n",
       "      <td>4.159500</td>\n",
       "      <td>62174.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>4.159000</td>\n",
       "      <td>4.194500</td>\n",
       "      <td>4.120500</td>\n",
       "      <td>4.139500</td>\n",
       "      <td>4.139500</td>\n",
       "      <td>43274.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>4.104000</td>\n",
       "      <td>4.122500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>4.041500</td>\n",
       "      <td>4.041500</td>\n",
       "      <td>256462.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>4.079000</td>\n",
       "      <td>4.102500</td>\n",
       "      <td>3.965000</td>\n",
       "      <td>3.989000</td>\n",
       "      <td>3.989000</td>\n",
       "      <td>79591.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>4.058000</td>\n",
       "      <td>4.112500</td>\n",
       "      <td>4.017000</td>\n",
       "      <td>4.036500</td>\n",
       "      <td>4.036500</td>\n",
       "      <td>100434.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>4.045500</td>\n",
       "      <td>4.055000</td>\n",
       "      <td>3.965500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>71445.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>4.002000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>55739.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.980000</td>\n",
       "      <td>4.053000</td>\n",
       "      <td>3.971500</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>79988.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>4.060000</td>\n",
       "      <td>4.137500</td>\n",
       "      <td>4.060000</td>\n",
       "      <td>4.111500</td>\n",
       "      <td>4.111500</td>\n",
       "      <td>122889.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>4.120000</td>\n",
       "      <td>4.152000</td>\n",
       "      <td>4.090500</td>\n",
       "      <td>4.101500</td>\n",
       "      <td>4.101500</td>\n",
       "      <td>50662.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>4.115500</td>\n",
       "      <td>4.155000</td>\n",
       "      <td>4.059500</td>\n",
       "      <td>4.068500</td>\n",
       "      <td>4.068500</td>\n",
       "      <td>61511.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>4.070000</td>\n",
       "      <td>4.098500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.019500</td>\n",
       "      <td>4.019500</td>\n",
       "      <td>51406.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>4.040000</td>\n",
       "      <td>4.045500</td>\n",
       "      <td>3.982000</td>\n",
       "      <td>4.024000</td>\n",
       "      <td>4.024000</td>\n",
       "      <td>46740.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>4.065000</td>\n",
       "      <td>4.090000</td>\n",
       "      <td>4.042000</td>\n",
       "      <td>4.051000</td>\n",
       "      <td>4.051000</td>\n",
       "      <td>56141.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>3.990000</td>\n",
       "      <td>4.099000</td>\n",
       "      <td>3.893000</td>\n",
       "      <td>4.058000</td>\n",
       "      <td>4.058000</td>\n",
       "      <td>53562.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>4.073500</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>4.073500</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>96868.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>4.219500</td>\n",
       "      <td>4.277500</td>\n",
       "      <td>4.192500</td>\n",
       "      <td>4.226000</td>\n",
       "      <td>4.226000</td>\n",
       "      <td>111911.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>4.240000</td>\n",
       "      <td>4.260500</td>\n",
       "      <td>4.195500</td>\n",
       "      <td>4.208000</td>\n",
       "      <td>4.208000</td>\n",
       "      <td>48852.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>4.220500</td>\n",
       "      <td>4.264000</td>\n",
       "      <td>4.209000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>53452.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>4.220000</td>\n",
       "      <td>4.251500</td>\n",
       "      <td>4.211500</td>\n",
       "      <td>4.242000</td>\n",
       "      <td>4.242000</td>\n",
       "      <td>37006.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>4.235000</td>\n",
       "      <td>4.284500</td>\n",
       "      <td>4.203000</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>36733.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>4.235000</td>\n",
       "      <td>4.237000</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>4.186000</td>\n",
       "      <td>4.186000</td>\n",
       "      <td>50188.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>4.180000</td>\n",
       "      <td>4.334500</td>\n",
       "      <td>4.171500</td>\n",
       "      <td>4.318500</td>\n",
       "      <td>4.318500</td>\n",
       "      <td>86409.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5196 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close     Volume\n",
       "Date                                                                    \n",
       "1997-01-01  0.657293  0.691786  0.655377  0.690637   0.156121  116802.85\n",
       "1997-01-02  0.695619  0.695619  0.676456  0.683642   0.154540  128507.66\n",
       "1997-01-03  0.676456  0.689774  0.674827  0.687092   0.155320   79989.83\n",
       "1997-01-06  0.680289  0.690349  0.672815  0.677414   0.153132   56131.43\n",
       "1997-01-07  0.675977  0.675977  0.653939  0.659113   0.148995  114660.71\n",
       "1997-01-08  0.669749  0.684121  0.665437  0.670036   0.151464   52462.92\n",
       "1997-01-09  0.670036  0.675498  0.663042  0.667928   0.150987   56314.08\n",
       "1997-01-10  0.674540  0.689678  0.672815  0.685175   0.154886   97257.43\n",
       "1997-01-13  0.680289  0.680289  0.669366  0.676456   0.152915   42305.35\n",
       "1997-01-14  0.680289  0.683546  0.657389  0.664000   0.150099   65088.77\n",
       "1997-01-15  0.670803  0.710566  0.670707  0.710375   0.160583   84981.21\n",
       "1997-01-16  0.720531  0.758665  0.666874  0.677510   0.153153  113797.07\n",
       "1997-01-17  0.661796  0.682109  0.643017  0.662179   0.149688  103796.05\n",
       "1997-01-20  0.661126  0.662850  0.638609  0.641483   0.145009   51001.77\n",
       "1997-01-21  0.637172  0.668024  0.632956  0.641771   0.145075   81317.91\n",
       "1997-01-22  0.653173  0.662084  0.632860  0.639759   0.144620   81056.99\n",
       "1997-01-24  0.672815  0.672815  0.626153  0.641771   0.145075   93348.87\n",
       "1997-01-27  0.686708  0.686708  0.647903  0.677989   0.153262  131776.97\n",
       "1997-01-28  0.683930  0.689870  0.662179  0.676360   0.152894   84234.98\n",
       "1997-01-29  0.687954  0.698398  0.654418  0.664671   0.150251   66964.78\n",
       "1997-01-30  0.661126  0.662563  0.638226  0.641196   0.144945   65678.45\n",
       "1997-01-31  0.628548  0.632381  0.609385  0.615038   0.139031   58224.00\n",
       "1997-02-03  0.599900  0.612451  0.584569  0.589743   0.133313   55484.35\n",
       "1997-02-04  0.586390  0.599804  0.580832  0.589360   0.133227   74941.06\n",
       "1997-02-05  0.605457  0.616763  0.597887  0.599516   0.135523   38882.10\n",
       "1997-02-06  0.598750  0.613026  0.598750  0.608523   0.137559   26402.36\n",
       "1997-02-07  0.611206  0.624716  0.601720  0.612739   0.138512   35743.25\n",
       "1997-02-10  0.615134  0.615709  0.606128  0.610822   0.138078   20894.37\n",
       "1997-02-11  0.612260  0.640909  0.608427  0.635447   0.143645   38670.76\n",
       "1997-02-12  0.631327  0.649628  0.628644  0.638321   0.144295   28267.93\n",
       "...              ...       ...       ...       ...        ...        ...\n",
       "2017-11-17  4.194500  4.234500  4.170000  4.218000   4.218000   61914.28\n",
       "2017-11-20  4.210000  4.257000  4.201000  4.230000   4.230000   40511.53\n",
       "2017-11-21  4.244500  4.300000  4.225000  4.242500   4.242500   67454.07\n",
       "2017-11-22  4.254500  4.330000  4.245000  4.286000   4.286000   78994.17\n",
       "2017-11-23  4.286000  4.310500  4.224500  4.262500   4.262500   73742.16\n",
       "2017-11-24  4.270000  4.299500  4.242500  4.255000   4.255000   51081.82\n",
       "2017-11-27  4.230000  4.247500  4.178000  4.214000   4.214000   79460.28\n",
       "2017-11-28  4.198000  4.222500  4.145000  4.159500   4.159500   62174.80\n",
       "2017-11-29  4.159000  4.194500  4.120500  4.139500   4.139500   43274.53\n",
       "2017-11-30  4.104000  4.122500  4.022500  4.041500   4.041500  256462.62\n",
       "2017-12-01  4.079000  4.102500  3.965000  3.989000   3.989000   79591.97\n",
       "2017-12-04  4.058000  4.112500  4.017000  4.036500   4.036500  100434.91\n",
       "2017-12-05  4.045500  4.055000  3.965500  4.022500   4.022500   71445.42\n",
       "2017-12-06  4.002000  4.040000  3.950000  3.970500   3.970500   55739.68\n",
       "2017-12-07  3.980000  4.053000  3.971500  4.020000   4.020000   79988.63\n",
       "2017-12-08  4.060000  4.137500  4.060000  4.111500   4.111500  122889.46\n",
       "2017-12-11  4.120000  4.152000  4.090500  4.101500   4.101500   50662.42\n",
       "2017-12-12  4.115500  4.155000  4.059500  4.068500   4.068500   61511.97\n",
       "2017-12-13  4.070000  4.098500  4.000000  4.019500   4.019500   51406.89\n",
       "2017-12-14  4.040000  4.045500  3.982000  4.024000   4.024000   46740.26\n",
       "2017-12-15  4.065000  4.090000  4.042000  4.051000   4.051000   56141.13\n",
       "2017-12-18  3.990000  4.099000  3.893000  4.058000   4.058000   53562.08\n",
       "2017-12-19  4.073500  4.217000  4.073500  4.201000   4.201000   96868.83\n",
       "2017-12-20  4.219500  4.277500  4.192500  4.226000   4.226000  111911.27\n",
       "2017-12-21  4.240000  4.260500  4.195500  4.208000   4.208000   48852.25\n",
       "2017-12-22  4.220500  4.264000  4.209000  4.220000   4.220000   53452.61\n",
       "2017-12-26  4.220000  4.251500  4.211500  4.242000   4.242000   37006.55\n",
       "2017-12-27  4.235000  4.284500  4.203000  4.224500   4.224500   36733.55\n",
       "2017-12-28  4.235000  4.237000  4.160000  4.186000   4.186000   50188.93\n",
       "2017-12-29  4.180000  4.334500  4.171500  4.318500   4.318500   86409.08\n",
       "\n",
       "[5196 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4, 512)            2560      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4, 128)            65664     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sabrish/Documents/Training/machine-learning-master/capstone/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2076 samples, validate on 2077 samples\n",
      "Epoch 1/500\n",
      "2076/2076 [==============================] - 1s 694us/step - loss: 0.0080 - mean_absolute_percentage_error: 10.3582 - mean_absolute_error: 0.0427 - val_loss: 0.0052 - val_mean_absolute_percentage_error: 3.5721 - val_mean_absolute_error: 0.0530\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00515, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 2/500\n",
      "2076/2076 [==============================] - 1s 523us/step - loss: 5.2242e-04 - mean_absolute_percentage_error: 3.8909 - mean_absolute_error: 0.0161 - val_loss: 0.0036 - val_mean_absolute_percentage_error: 2.9716 - val_mean_absolute_error: 0.0439\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00515 to 0.00360, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 3/500\n",
      "2076/2076 [==============================] - 1s 489us/step - loss: 4.2323e-04 - mean_absolute_percentage_error: 3.5784 - mean_absolute_error: 0.0146 - val_loss: 0.0041 - val_mean_absolute_percentage_error: 3.0923 - val_mean_absolute_error: 0.0468\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00360\n",
      "Epoch 4/500\n",
      "2076/2076 [==============================] - 1s 482us/step - loss: 4.0801e-04 - mean_absolute_percentage_error: 3.5354 - mean_absolute_error: 0.0144 - val_loss: 0.0035 - val_mean_absolute_percentage_error: 2.9191 - val_mean_absolute_error: 0.0433\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00360 to 0.00349, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 5/500\n",
      "2076/2076 [==============================] - 1s 497us/step - loss: 4.2407e-04 - mean_absolute_percentage_error: 3.5604 - mean_absolute_error: 0.0148 - val_loss: 0.0031 - val_mean_absolute_percentage_error: 2.6938 - val_mean_absolute_error: 0.0405\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00349 to 0.00307, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 6/500\n",
      "2076/2076 [==============================] - 1s 499us/step - loss: 4.5641e-04 - mean_absolute_percentage_error: 3.7044 - mean_absolute_error: 0.0153 - val_loss: 0.0042 - val_mean_absolute_percentage_error: 3.1233 - val_mean_absolute_error: 0.0488\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00307\n",
      "Epoch 7/500\n",
      "2076/2076 [==============================] - 1s 506us/step - loss: 3.7280e-04 - mean_absolute_percentage_error: 3.3506 - mean_absolute_error: 0.0138 - val_loss: 0.0041 - val_mean_absolute_percentage_error: 3.1043 - val_mean_absolute_error: 0.0482\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00307\n",
      "Epoch 8/500\n",
      "2076/2076 [==============================] - 1s 504us/step - loss: 6.5432e-04 - mean_absolute_percentage_error: 4.4121 - mean_absolute_error: 0.0187 - val_loss: 0.0029 - val_mean_absolute_percentage_error: 2.6082 - val_mean_absolute_error: 0.0389\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00307 to 0.00287, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 9/500\n",
      "2076/2076 [==============================] - 1s 528us/step - loss: 3.9752e-04 - mean_absolute_percentage_error: 3.4534 - mean_absolute_error: 0.0143 - val_loss: 0.0035 - val_mean_absolute_percentage_error: 2.9564 - val_mean_absolute_error: 0.0443\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00287\n",
      "Epoch 10/500\n",
      "2076/2076 [==============================] - 1s 582us/step - loss: 3.6578e-04 - mean_absolute_percentage_error: 3.3089 - mean_absolute_error: 0.0136 - val_loss: 0.0025 - val_mean_absolute_percentage_error: 2.4406 - val_mean_absolute_error: 0.0362\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00287 to 0.00251, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 11/500\n",
      "2076/2076 [==============================] - 1s 595us/step - loss: 4.3757e-04 - mean_absolute_percentage_error: 3.6015 - mean_absolute_error: 0.0151 - val_loss: 0.0051 - val_mean_absolute_percentage_error: 3.4166 - val_mean_absolute_error: 0.0549\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00251\n",
      "Epoch 12/500\n",
      "2076/2076 [==============================] - 1s 545us/step - loss: 3.8732e-04 - mean_absolute_percentage_error: 3.4334 - mean_absolute_error: 0.0141 - val_loss: 0.0091 - val_mean_absolute_percentage_error: 4.7216 - val_mean_absolute_error: 0.0769\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00251\n",
      "Epoch 13/500\n",
      "2076/2076 [==============================] - 1s 524us/step - loss: 6.1950e-04 - mean_absolute_percentage_error: 4.2526 - mean_absolute_error: 0.0180 - val_loss: 0.0024 - val_mean_absolute_percentage_error: 2.3795 - val_mean_absolute_error: 0.0355\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00251 to 0.00240, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 14/500\n",
      "2076/2076 [==============================] - 1s 574us/step - loss: 4.0279e-04 - mean_absolute_percentage_error: 3.4858 - mean_absolute_error: 0.0146 - val_loss: 0.0069 - val_mean_absolute_percentage_error: 4.1737 - val_mean_absolute_error: 0.0662\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00240\n",
      "Epoch 15/500\n",
      "2076/2076 [==============================] - 1s 521us/step - loss: 4.2672e-04 - mean_absolute_percentage_error: 3.6071 - mean_absolute_error: 0.0150 - val_loss: 0.0028 - val_mean_absolute_percentage_error: 2.5278 - val_mean_absolute_error: 0.0387\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00240\n",
      "Epoch 16/500\n",
      "2076/2076 [==============================] - 1s 538us/step - loss: 5.1649e-04 - mean_absolute_percentage_error: 3.7572 - mean_absolute_error: 0.0161 - val_loss: 0.0069 - val_mean_absolute_percentage_error: 4.1936 - val_mean_absolute_error: 0.0663\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00240\n",
      "Epoch 17/500\n",
      "2076/2076 [==============================] - 1s 535us/step - loss: 4.3411e-04 - mean_absolute_percentage_error: 3.5944 - mean_absolute_error: 0.0151 - val_loss: 0.0053 - val_mean_absolute_percentage_error: 3.5123 - val_mean_absolute_error: 0.0560\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00240\n",
      "Epoch 18/500\n",
      "2076/2076 [==============================] - 1s 523us/step - loss: 3.5462e-04 - mean_absolute_percentage_error: 3.2307 - mean_absolute_error: 0.0133 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2525 - val_mean_absolute_error: 0.0336\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00240 to 0.00218, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 19/500\n",
      "2076/2076 [==============================] - 1s 490us/step - loss: 3.8650e-04 - mean_absolute_percentage_error: 3.3914 - mean_absolute_error: 0.0141 - val_loss: 0.0036 - val_mean_absolute_percentage_error: 2.8730 - val_mean_absolute_error: 0.0451\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00218\n",
      "Epoch 20/500\n",
      "2076/2076 [==============================] - 1s 481us/step - loss: 4.4950e-04 - mean_absolute_percentage_error: 3.5224 - mean_absolute_error: 0.0150 - val_loss: 0.0030 - val_mean_absolute_percentage_error: 2.6754 - val_mean_absolute_error: 0.0409\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00218\n",
      "Epoch 21/500\n",
      "2076/2076 [==============================] - 1s 495us/step - loss: 6.5083e-04 - mean_absolute_percentage_error: 4.0884 - mean_absolute_error: 0.0176 - val_loss: 0.0024 - val_mean_absolute_percentage_error: 2.3113 - val_mean_absolute_error: 0.0350\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00218\n",
      "Epoch 22/500\n",
      "2076/2076 [==============================] - 1s 473us/step - loss: 3.1166e-04 - mean_absolute_percentage_error: 3.0178 - mean_absolute_error: 0.0125 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2515 - val_mean_absolute_error: 0.0338\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00218\n",
      "Epoch 23/500\n",
      "2076/2076 [==============================] - 1s 511us/step - loss: 3.2383e-04 - mean_absolute_percentage_error: 3.1138 - mean_absolute_error: 0.0129 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.4656 - val_mean_absolute_error: 0.0378\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00218\n",
      "Epoch 24/500\n",
      "2076/2076 [==============================] - 1s 488us/step - loss: 3.8048e-04 - mean_absolute_percentage_error: 3.3967 - mean_absolute_error: 0.0140 - val_loss: 0.0035 - val_mean_absolute_percentage_error: 2.8718 - val_mean_absolute_error: 0.0445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00218\n",
      "Epoch 25/500\n",
      "2076/2076 [==============================] - 1s 495us/step - loss: 3.1941e-04 - mean_absolute_percentage_error: 3.1381 - mean_absolute_error: 0.0129 - val_loss: 0.0023 - val_mean_absolute_percentage_error: 2.2983 - val_mean_absolute_error: 0.0345\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00218\n",
      "Epoch 26/500\n",
      "2076/2076 [==============================] - 1s 488us/step - loss: 3.0553e-04 - mean_absolute_percentage_error: 3.0397 - mean_absolute_error: 0.0125 - val_loss: 0.0056 - val_mean_absolute_percentage_error: 3.6504 - val_mean_absolute_error: 0.0583\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00218\n",
      "Epoch 27/500\n",
      "2076/2076 [==============================] - 1s 489us/step - loss: 3.4308e-04 - mean_absolute_percentage_error: 3.1897 - mean_absolute_error: 0.0132 - val_loss: 0.0063 - val_mean_absolute_percentage_error: 3.8351 - val_mean_absolute_error: 0.0632\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00218\n",
      "Epoch 28/500\n",
      "2076/2076 [==============================] - 1s 504us/step - loss: 4.3200e-04 - mean_absolute_percentage_error: 3.5181 - mean_absolute_error: 0.0147 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.3950 - val_mean_absolute_error: 0.0369\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00218\n",
      "Epoch 29/500\n",
      "2076/2076 [==============================] - 1s 494us/step - loss: 5.5065e-04 - mean_absolute_percentage_error: 3.9224 - mean_absolute_error: 0.0169 - val_loss: 0.0048 - val_mean_absolute_percentage_error: 3.3928 - val_mean_absolute_error: 0.0540\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00218\n",
      "Epoch 30/500\n",
      "2076/2076 [==============================] - 1s 569us/step - loss: 3.2547e-04 - mean_absolute_percentage_error: 3.1987 - mean_absolute_error: 0.0132 - val_loss: 0.0066 - val_mean_absolute_percentage_error: 4.0232 - val_mean_absolute_error: 0.0644\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00218\n",
      "Epoch 31/500\n",
      "2076/2076 [==============================] - 1s 562us/step - loss: 6.1329e-04 - mean_absolute_percentage_error: 4.2030 - mean_absolute_error: 0.0180 - val_loss: 0.0139 - val_mean_absolute_percentage_error: 5.7788 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00218\n",
      "Epoch 32/500\n",
      "2076/2076 [==============================] - 1s 553us/step - loss: 3.6076e-04 - mean_absolute_percentage_error: 3.3730 - mean_absolute_error: 0.0137 - val_loss: 0.0066 - val_mean_absolute_percentage_error: 4.0755 - val_mean_absolute_error: 0.0657\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00218\n",
      "Epoch 33/500\n",
      "2076/2076 [==============================] - 1s 547us/step - loss: 3.3394e-04 - mean_absolute_percentage_error: 3.2075 - mean_absolute_error: 0.0131 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.4170 - val_mean_absolute_error: 0.0372\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00218\n",
      "Epoch 34/500\n",
      "2076/2076 [==============================] - 1s 505us/step - loss: 3.0983e-04 - mean_absolute_percentage_error: 3.0713 - mean_absolute_error: 0.0126 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.4937 - val_mean_absolute_error: 0.0377\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00218\n",
      "Epoch 35/500\n",
      "2076/2076 [==============================] - 1s 498us/step - loss: 2.5939e-04 - mean_absolute_percentage_error: 2.8347 - mean_absolute_error: 0.0115 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.1963 - val_mean_absolute_error: 0.0329\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00218 to 0.00209, saving model to TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 36/500\n",
      "2076/2076 [==============================] - 1s 484us/step - loss: 4.0644e-04 - mean_absolute_percentage_error: 3.3596 - mean_absolute_error: 0.0140 - val_loss: 0.0025 - val_mean_absolute_percentage_error: 2.4163 - val_mean_absolute_error: 0.0364\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00209\n",
      "Epoch 37/500\n",
      "2076/2076 [==============================] - 1s 488us/step - loss: 3.2990e-04 - mean_absolute_percentage_error: 3.1590 - mean_absolute_error: 0.0130 - val_loss: 0.0116 - val_mean_absolute_percentage_error: 5.3339 - val_mean_absolute_error: 0.0900\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00209\n",
      "Epoch 38/500\n",
      "2076/2076 [==============================] - 1s 482us/step - loss: 3.8849e-04 - mean_absolute_percentage_error: 3.3216 - mean_absolute_error: 0.0140 - val_loss: 0.0063 - val_mean_absolute_percentage_error: 3.8770 - val_mean_absolute_error: 0.0625\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00209\n",
      "Epoch 39/500\n",
      "2076/2076 [==============================] - 1s 482us/step - loss: 4.3405e-04 - mean_absolute_percentage_error: 3.4856 - mean_absolute_error: 0.0148 - val_loss: 0.0037 - val_mean_absolute_percentage_error: 2.8474 - val_mean_absolute_error: 0.0457\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00209\n",
      "Epoch 40/500\n",
      "2076/2076 [==============================] - 1s 465us/step - loss: 3.7803e-04 - mean_absolute_percentage_error: 3.4775 - mean_absolute_error: 0.0143 - val_loss: 0.0070 - val_mean_absolute_percentage_error: 4.0879 - val_mean_absolute_error: 0.0663\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00209\n",
      "Epoch 41/500\n",
      "2076/2076 [==============================] - 1s 477us/step - loss: 3.8674e-04 - mean_absolute_percentage_error: 3.4023 - mean_absolute_error: 0.0142 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2260 - val_mean_absolute_error: 0.0336\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00209\n",
      "Epoch 42/500\n",
      "2076/2076 [==============================] - 1s 510us/step - loss: 3.3294e-04 - mean_absolute_percentage_error: 3.2015 - mean_absolute_error: 0.0131 - val_loss: 0.0035 - val_mean_absolute_percentage_error: 2.8235 - val_mean_absolute_error: 0.0446\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00209\n",
      "Epoch 43/500\n",
      "2076/2076 [==============================] - 1s 490us/step - loss: 3.4476e-04 - mean_absolute_percentage_error: 3.2524 - mean_absolute_error: 0.0134 - val_loss: 0.0029 - val_mean_absolute_percentage_error: 2.5770 - val_mean_absolute_error: 0.0400\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00209\n",
      "Epoch 44/500\n",
      "2076/2076 [==============================] - 1s 485us/step - loss: 3.0047e-04 - mean_absolute_percentage_error: 3.0395 - mean_absolute_error: 0.0125 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2061 - val_mean_absolute_error: 0.0332\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00209\n",
      "Epoch 45/500\n",
      "2076/2076 [==============================] - 1s 491us/step - loss: 3.0886e-04 - mean_absolute_percentage_error: 3.0909 - mean_absolute_error: 0.0126 - val_loss: 0.0037 - val_mean_absolute_percentage_error: 2.9457 - val_mean_absolute_error: 0.0457\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00209\n",
      "Epoch 46/500\n",
      "2076/2076 [==============================] - 1s 470us/step - loss: 2.9122e-04 - mean_absolute_percentage_error: 2.9710 - mean_absolute_error: 0.0122 - val_loss: 0.0030 - val_mean_absolute_percentage_error: 2.5750 - val_mean_absolute_error: 0.0405\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00209\n",
      "Epoch 47/500\n",
      "2076/2076 [==============================] - 1s 485us/step - loss: 3.3370e-04 - mean_absolute_percentage_error: 3.2255 - mean_absolute_error: 0.0132 - val_loss: 0.0048 - val_mean_absolute_percentage_error: 3.2468 - val_mean_absolute_error: 0.0534\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00209\n",
      "Epoch 48/500\n",
      "2076/2076 [==============================] - 1s 468us/step - loss: 3.1786e-04 - mean_absolute_percentage_error: 3.1976 - mean_absolute_error: 0.0129 - val_loss: 0.0031 - val_mean_absolute_percentage_error: 2.7178 - val_mean_absolute_error: 0.0420\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00209\n",
      "Epoch 49/500\n",
      "2076/2076 [==============================] - 1s 486us/step - loss: 4.3269e-04 - mean_absolute_percentage_error: 3.5510 - mean_absolute_error: 0.0150 - val_loss: 0.0024 - val_mean_absolute_percentage_error: 2.3146 - val_mean_absolute_error: 0.0352\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00209\n",
      "Epoch 50/500\n",
      "2076/2076 [==============================] - 1s 485us/step - loss: 2.7756e-04 - mean_absolute_percentage_error: 2.9376 - mean_absolute_error: 0.0119 - val_loss: 0.0031 - val_mean_absolute_percentage_error: 2.6246 - val_mean_absolute_error: 0.0412\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00209\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - 1s 467us/step - loss: 2.5996e-04 - mean_absolute_percentage_error: 2.8927 - mean_absolute_error: 0.0116 - val_loss: 0.0068 - val_mean_absolute_percentage_error: 3.9975 - val_mean_absolute_error: 0.0664\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00209\n",
      "Epoch 52/500\n",
      "2076/2076 [==============================] - 1s 481us/step - loss: 4.3350e-04 - mean_absolute_percentage_error: 3.6595 - mean_absolute_error: 0.0152 - val_loss: 0.0029 - val_mean_absolute_percentage_error: 2.5191 - val_mean_absolute_error: 0.0396\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00209\n",
      "Epoch 53/500\n",
      "2076/2076 [==============================] - 1s 487us/step - loss: 3.4402e-04 - mean_absolute_percentage_error: 3.2644 - mean_absolute_error: 0.0134 - val_loss: 0.0030 - val_mean_absolute_percentage_error: 2.6236 - val_mean_absolute_error: 0.0408\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00209\n",
      "Epoch 54/500\n",
      "2076/2076 [==============================] - 1s 486us/step - loss: 3.7788e-04 - mean_absolute_percentage_error: 3.4126 - mean_absolute_error: 0.0142 - val_loss: 0.0033 - val_mean_absolute_percentage_error: 2.6970 - val_mean_absolute_error: 0.0426\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00209\n",
      "Epoch 55/500\n",
      "2076/2076 [==============================] - 1s 490us/step - loss: 3.7372e-04 - mean_absolute_percentage_error: 3.3989 - mean_absolute_error: 0.0141 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2543 - val_mean_absolute_error: 0.0342\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1337f1438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039/1039 [==============================] - 0s 99us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008974134562137295, 1.6339636579858672, 0.0710503869224788]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
