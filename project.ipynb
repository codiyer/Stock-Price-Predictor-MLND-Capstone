{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrival and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'TATAMOTORS.NS'\n",
    "num_of_days = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "from models.MLPModel import MLPModel\n",
    "\n",
    "model = MLPModel(ticker, num_of_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n",
      "WARNING:tensorflow:From /Users/sabrish/Documents/Training/machine-learning-master/capstone/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from models.LSTMModel import LSTMModel\n",
    "\n",
    "model = LSTMModel(ticker, num_of_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-01-01</th>\n",
       "      <td>0.657293</td>\n",
       "      <td>0.691786</td>\n",
       "      <td>0.655377</td>\n",
       "      <td>0.690637</td>\n",
       "      <td>0.156121</td>\n",
       "      <td>116802.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-02</th>\n",
       "      <td>0.695619</td>\n",
       "      <td>0.695619</td>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.683642</td>\n",
       "      <td>0.154540</td>\n",
       "      <td>128507.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-03</th>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.689774</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.155320</td>\n",
       "      <td>79989.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-06</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.677414</td>\n",
       "      <td>0.153132</td>\n",
       "      <td>56131.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-07</th>\n",
       "      <td>0.675977</td>\n",
       "      <td>0.675977</td>\n",
       "      <td>0.653939</td>\n",
       "      <td>0.659113</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>114660.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-08</th>\n",
       "      <td>0.669749</td>\n",
       "      <td>0.684121</td>\n",
       "      <td>0.665437</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.151464</td>\n",
       "      <td>52462.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-09</th>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.675498</td>\n",
       "      <td>0.663042</td>\n",
       "      <td>0.667928</td>\n",
       "      <td>0.150987</td>\n",
       "      <td>56314.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-10</th>\n",
       "      <td>0.674540</td>\n",
       "      <td>0.689678</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.685175</td>\n",
       "      <td>0.154886</td>\n",
       "      <td>97257.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-13</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.669366</td>\n",
       "      <td>0.676456</td>\n",
       "      <td>0.152915</td>\n",
       "      <td>42305.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-14</th>\n",
       "      <td>0.680289</td>\n",
       "      <td>0.683546</td>\n",
       "      <td>0.657389</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.150099</td>\n",
       "      <td>65088.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-15</th>\n",
       "      <td>0.670803</td>\n",
       "      <td>0.710566</td>\n",
       "      <td>0.670707</td>\n",
       "      <td>0.710375</td>\n",
       "      <td>0.160583</td>\n",
       "      <td>84981.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-16</th>\n",
       "      <td>0.720531</td>\n",
       "      <td>0.758665</td>\n",
       "      <td>0.666874</td>\n",
       "      <td>0.677510</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>113797.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-17</th>\n",
       "      <td>0.661796</td>\n",
       "      <td>0.682109</td>\n",
       "      <td>0.643017</td>\n",
       "      <td>0.662179</td>\n",
       "      <td>0.149688</td>\n",
       "      <td>103796.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-20</th>\n",
       "      <td>0.661126</td>\n",
       "      <td>0.662850</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>0.641483</td>\n",
       "      <td>0.145009</td>\n",
       "      <td>51001.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-21</th>\n",
       "      <td>0.637172</td>\n",
       "      <td>0.668024</td>\n",
       "      <td>0.632956</td>\n",
       "      <td>0.641771</td>\n",
       "      <td>0.145075</td>\n",
       "      <td>81317.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-22</th>\n",
       "      <td>0.653173</td>\n",
       "      <td>0.662084</td>\n",
       "      <td>0.632860</td>\n",
       "      <td>0.639759</td>\n",
       "      <td>0.144620</td>\n",
       "      <td>81056.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-24</th>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.672815</td>\n",
       "      <td>0.626153</td>\n",
       "      <td>0.641771</td>\n",
       "      <td>0.145075</td>\n",
       "      <td>93348.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-27</th>\n",
       "      <td>0.686708</td>\n",
       "      <td>0.686708</td>\n",
       "      <td>0.647903</td>\n",
       "      <td>0.677989</td>\n",
       "      <td>0.153262</td>\n",
       "      <td>131776.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-28</th>\n",
       "      <td>0.683930</td>\n",
       "      <td>0.689870</td>\n",
       "      <td>0.662179</td>\n",
       "      <td>0.676360</td>\n",
       "      <td>0.152894</td>\n",
       "      <td>84234.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-29</th>\n",
       "      <td>0.687954</td>\n",
       "      <td>0.698398</td>\n",
       "      <td>0.654418</td>\n",
       "      <td>0.664671</td>\n",
       "      <td>0.150251</td>\n",
       "      <td>66964.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-30</th>\n",
       "      <td>0.661126</td>\n",
       "      <td>0.662563</td>\n",
       "      <td>0.638226</td>\n",
       "      <td>0.641196</td>\n",
       "      <td>0.144945</td>\n",
       "      <td>65678.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-01-31</th>\n",
       "      <td>0.628548</td>\n",
       "      <td>0.632381</td>\n",
       "      <td>0.609385</td>\n",
       "      <td>0.615038</td>\n",
       "      <td>0.139031</td>\n",
       "      <td>58224.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-03</th>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.612451</td>\n",
       "      <td>0.584569</td>\n",
       "      <td>0.589743</td>\n",
       "      <td>0.133313</td>\n",
       "      <td>55484.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-04</th>\n",
       "      <td>0.586390</td>\n",
       "      <td>0.599804</td>\n",
       "      <td>0.580832</td>\n",
       "      <td>0.589360</td>\n",
       "      <td>0.133227</td>\n",
       "      <td>74941.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-05</th>\n",
       "      <td>0.605457</td>\n",
       "      <td>0.616763</td>\n",
       "      <td>0.597887</td>\n",
       "      <td>0.599516</td>\n",
       "      <td>0.135523</td>\n",
       "      <td>38882.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-06</th>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.613026</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.608523</td>\n",
       "      <td>0.137559</td>\n",
       "      <td>26402.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-07</th>\n",
       "      <td>0.611206</td>\n",
       "      <td>0.624716</td>\n",
       "      <td>0.601720</td>\n",
       "      <td>0.612739</td>\n",
       "      <td>0.138512</td>\n",
       "      <td>35743.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-10</th>\n",
       "      <td>0.615134</td>\n",
       "      <td>0.615709</td>\n",
       "      <td>0.606128</td>\n",
       "      <td>0.610822</td>\n",
       "      <td>0.138078</td>\n",
       "      <td>20894.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-11</th>\n",
       "      <td>0.612260</td>\n",
       "      <td>0.640909</td>\n",
       "      <td>0.608427</td>\n",
       "      <td>0.635447</td>\n",
       "      <td>0.143645</td>\n",
       "      <td>38670.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-02-12</th>\n",
       "      <td>0.631327</td>\n",
       "      <td>0.649628</td>\n",
       "      <td>0.628644</td>\n",
       "      <td>0.638321</td>\n",
       "      <td>0.144295</td>\n",
       "      <td>28267.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>4.194500</td>\n",
       "      <td>4.234500</td>\n",
       "      <td>4.170000</td>\n",
       "      <td>4.218000</td>\n",
       "      <td>4.218000</td>\n",
       "      <td>61914.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>4.210000</td>\n",
       "      <td>4.257000</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>40511.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>4.244500</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>67454.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>4.254500</td>\n",
       "      <td>4.330000</td>\n",
       "      <td>4.245000</td>\n",
       "      <td>4.286000</td>\n",
       "      <td>4.286000</td>\n",
       "      <td>78994.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>4.286000</td>\n",
       "      <td>4.310500</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>4.262500</td>\n",
       "      <td>4.262500</td>\n",
       "      <td>73742.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>4.270000</td>\n",
       "      <td>4.299500</td>\n",
       "      <td>4.242500</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>51081.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>4.230000</td>\n",
       "      <td>4.247500</td>\n",
       "      <td>4.178000</td>\n",
       "      <td>4.214000</td>\n",
       "      <td>4.214000</td>\n",
       "      <td>79460.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>4.198000</td>\n",
       "      <td>4.222500</td>\n",
       "      <td>4.145000</td>\n",
       "      <td>4.159500</td>\n",
       "      <td>4.159500</td>\n",
       "      <td>62174.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>4.159000</td>\n",
       "      <td>4.194500</td>\n",
       "      <td>4.120500</td>\n",
       "      <td>4.139500</td>\n",
       "      <td>4.139500</td>\n",
       "      <td>43274.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>4.104000</td>\n",
       "      <td>4.122500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>4.041500</td>\n",
       "      <td>4.041500</td>\n",
       "      <td>256462.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>4.079000</td>\n",
       "      <td>4.102500</td>\n",
       "      <td>3.965000</td>\n",
       "      <td>3.989000</td>\n",
       "      <td>3.989000</td>\n",
       "      <td>79591.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>4.058000</td>\n",
       "      <td>4.112500</td>\n",
       "      <td>4.017000</td>\n",
       "      <td>4.036500</td>\n",
       "      <td>4.036500</td>\n",
       "      <td>100434.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>4.045500</td>\n",
       "      <td>4.055000</td>\n",
       "      <td>3.965500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>4.022500</td>\n",
       "      <td>71445.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>4.002000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>55739.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.980000</td>\n",
       "      <td>4.053000</td>\n",
       "      <td>3.971500</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>79988.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>4.060000</td>\n",
       "      <td>4.137500</td>\n",
       "      <td>4.060000</td>\n",
       "      <td>4.111500</td>\n",
       "      <td>4.111500</td>\n",
       "      <td>122889.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>4.120000</td>\n",
       "      <td>4.152000</td>\n",
       "      <td>4.090500</td>\n",
       "      <td>4.101500</td>\n",
       "      <td>4.101500</td>\n",
       "      <td>50662.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>4.115500</td>\n",
       "      <td>4.155000</td>\n",
       "      <td>4.059500</td>\n",
       "      <td>4.068500</td>\n",
       "      <td>4.068500</td>\n",
       "      <td>61511.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>4.070000</td>\n",
       "      <td>4.098500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.019500</td>\n",
       "      <td>4.019500</td>\n",
       "      <td>51406.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>4.040000</td>\n",
       "      <td>4.045500</td>\n",
       "      <td>3.982000</td>\n",
       "      <td>4.024000</td>\n",
       "      <td>4.024000</td>\n",
       "      <td>46740.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>4.065000</td>\n",
       "      <td>4.090000</td>\n",
       "      <td>4.042000</td>\n",
       "      <td>4.051000</td>\n",
       "      <td>4.051000</td>\n",
       "      <td>56141.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>3.990000</td>\n",
       "      <td>4.099000</td>\n",
       "      <td>3.893000</td>\n",
       "      <td>4.058000</td>\n",
       "      <td>4.058000</td>\n",
       "      <td>53562.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>4.073500</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>4.073500</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>4.201000</td>\n",
       "      <td>96868.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>4.219500</td>\n",
       "      <td>4.277500</td>\n",
       "      <td>4.192500</td>\n",
       "      <td>4.226000</td>\n",
       "      <td>4.226000</td>\n",
       "      <td>111911.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>4.240000</td>\n",
       "      <td>4.260500</td>\n",
       "      <td>4.195500</td>\n",
       "      <td>4.208000</td>\n",
       "      <td>4.208000</td>\n",
       "      <td>48852.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>4.220500</td>\n",
       "      <td>4.264000</td>\n",
       "      <td>4.209000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>53452.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>4.220000</td>\n",
       "      <td>4.251500</td>\n",
       "      <td>4.211500</td>\n",
       "      <td>4.242000</td>\n",
       "      <td>4.242000</td>\n",
       "      <td>37006.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>4.235000</td>\n",
       "      <td>4.284500</td>\n",
       "      <td>4.203000</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>36733.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>4.235000</td>\n",
       "      <td>4.237000</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>4.186000</td>\n",
       "      <td>4.186000</td>\n",
       "      <td>50188.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>4.180000</td>\n",
       "      <td>4.334500</td>\n",
       "      <td>4.171500</td>\n",
       "      <td>4.318500</td>\n",
       "      <td>4.318500</td>\n",
       "      <td>86409.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5196 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close     Volume\n",
       "Date                                                                    \n",
       "1997-01-01  0.657293  0.691786  0.655377  0.690637   0.156121  116802.85\n",
       "1997-01-02  0.695619  0.695619  0.676456  0.683642   0.154540  128507.66\n",
       "1997-01-03  0.676456  0.689774  0.674827  0.687092   0.155320   79989.83\n",
       "1997-01-06  0.680289  0.690349  0.672815  0.677414   0.153132   56131.43\n",
       "1997-01-07  0.675977  0.675977  0.653939  0.659113   0.148995  114660.71\n",
       "1997-01-08  0.669749  0.684121  0.665437  0.670036   0.151464   52462.92\n",
       "1997-01-09  0.670036  0.675498  0.663042  0.667928   0.150987   56314.08\n",
       "1997-01-10  0.674540  0.689678  0.672815  0.685175   0.154886   97257.43\n",
       "1997-01-13  0.680289  0.680289  0.669366  0.676456   0.152915   42305.35\n",
       "1997-01-14  0.680289  0.683546  0.657389  0.664000   0.150099   65088.77\n",
       "1997-01-15  0.670803  0.710566  0.670707  0.710375   0.160583   84981.21\n",
       "1997-01-16  0.720531  0.758665  0.666874  0.677510   0.153153  113797.07\n",
       "1997-01-17  0.661796  0.682109  0.643017  0.662179   0.149688  103796.05\n",
       "1997-01-20  0.661126  0.662850  0.638609  0.641483   0.145009   51001.77\n",
       "1997-01-21  0.637172  0.668024  0.632956  0.641771   0.145075   81317.91\n",
       "1997-01-22  0.653173  0.662084  0.632860  0.639759   0.144620   81056.99\n",
       "1997-01-24  0.672815  0.672815  0.626153  0.641771   0.145075   93348.87\n",
       "1997-01-27  0.686708  0.686708  0.647903  0.677989   0.153262  131776.97\n",
       "1997-01-28  0.683930  0.689870  0.662179  0.676360   0.152894   84234.98\n",
       "1997-01-29  0.687954  0.698398  0.654418  0.664671   0.150251   66964.78\n",
       "1997-01-30  0.661126  0.662563  0.638226  0.641196   0.144945   65678.45\n",
       "1997-01-31  0.628548  0.632381  0.609385  0.615038   0.139031   58224.00\n",
       "1997-02-03  0.599900  0.612451  0.584569  0.589743   0.133313   55484.35\n",
       "1997-02-04  0.586390  0.599804  0.580832  0.589360   0.133227   74941.06\n",
       "1997-02-05  0.605457  0.616763  0.597887  0.599516   0.135523   38882.10\n",
       "1997-02-06  0.598750  0.613026  0.598750  0.608523   0.137559   26402.36\n",
       "1997-02-07  0.611206  0.624716  0.601720  0.612739   0.138512   35743.25\n",
       "1997-02-10  0.615134  0.615709  0.606128  0.610822   0.138078   20894.37\n",
       "1997-02-11  0.612260  0.640909  0.608427  0.635447   0.143645   38670.76\n",
       "1997-02-12  0.631327  0.649628  0.628644  0.638321   0.144295   28267.93\n",
       "...              ...       ...       ...       ...        ...        ...\n",
       "2017-11-17  4.194500  4.234500  4.170000  4.218000   4.218000   61914.28\n",
       "2017-11-20  4.210000  4.257000  4.201000  4.230000   4.230000   40511.53\n",
       "2017-11-21  4.244500  4.300000  4.225000  4.242500   4.242500   67454.07\n",
       "2017-11-22  4.254500  4.330000  4.245000  4.286000   4.286000   78994.17\n",
       "2017-11-23  4.286000  4.310500  4.224500  4.262500   4.262500   73742.16\n",
       "2017-11-24  4.270000  4.299500  4.242500  4.255000   4.255000   51081.82\n",
       "2017-11-27  4.230000  4.247500  4.178000  4.214000   4.214000   79460.28\n",
       "2017-11-28  4.198000  4.222500  4.145000  4.159500   4.159500   62174.80\n",
       "2017-11-29  4.159000  4.194500  4.120500  4.139500   4.139500   43274.53\n",
       "2017-11-30  4.104000  4.122500  4.022500  4.041500   4.041500  256462.62\n",
       "2017-12-01  4.079000  4.102500  3.965000  3.989000   3.989000   79591.97\n",
       "2017-12-04  4.058000  4.112500  4.017000  4.036500   4.036500  100434.91\n",
       "2017-12-05  4.045500  4.055000  3.965500  4.022500   4.022500   71445.42\n",
       "2017-12-06  4.002000  4.040000  3.950000  3.970500   3.970500   55739.68\n",
       "2017-12-07  3.980000  4.053000  3.971500  4.020000   4.020000   79988.63\n",
       "2017-12-08  4.060000  4.137500  4.060000  4.111500   4.111500  122889.46\n",
       "2017-12-11  4.120000  4.152000  4.090500  4.101500   4.101500   50662.42\n",
       "2017-12-12  4.115500  4.155000  4.059500  4.068500   4.068500   61511.97\n",
       "2017-12-13  4.070000  4.098500  4.000000  4.019500   4.019500   51406.89\n",
       "2017-12-14  4.040000  4.045500  3.982000  4.024000   4.024000   46740.26\n",
       "2017-12-15  4.065000  4.090000  4.042000  4.051000   4.051000   56141.13\n",
       "2017-12-18  3.990000  4.099000  3.893000  4.058000   4.058000   53562.08\n",
       "2017-12-19  4.073500  4.217000  4.073500  4.201000   4.201000   96868.83\n",
       "2017-12-20  4.219500  4.277500  4.192500  4.226000   4.226000  111911.27\n",
       "2017-12-21  4.240000  4.260500  4.195500  4.208000   4.208000   48852.25\n",
       "2017-12-22  4.220500  4.264000  4.209000  4.220000   4.220000   53452.61\n",
       "2017-12-26  4.220000  4.251500  4.211500  4.242000   4.242000   37006.55\n",
       "2017-12-27  4.235000  4.284500  4.203000  4.224500   4.224500   36733.55\n",
       "2017-12-28  4.235000  4.237000  4.160000  4.186000   4.186000   50188.93\n",
       "2017-12-29  4.180000  4.334500  4.171500  4.318500   4.318500   86409.08\n",
       "\n",
       "[5196 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 4, 512)            2560      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4, 512)            262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4, 128)            65664     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2076 samples, validate on 2077 samples\n",
      "Epoch 1/500\n",
      "2076/2076 [==============================] - 2s 827us/step - loss: 0.0084 - mean_absolute_percentage_error: 11.7563 - mean_absolute_error: 0.0472 - val_loss: 0.0053 - val_mean_absolute_percentage_error: 3.5957 - val_mean_absolute_error: 0.0543\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00533, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 2/500\n",
      "2076/2076 [==============================] - 1s 480us/step - loss: 5.5250e-04 - mean_absolute_percentage_error: 4.0425 - mean_absolute_error: 0.0167 - val_loss: 0.0047 - val_mean_absolute_percentage_error: 3.3457 - val_mean_absolute_error: 0.0506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00533 to 0.00468, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 3/500\n",
      "2076/2076 [==============================] - 1s 477us/step - loss: 5.6138e-04 - mean_absolute_percentage_error: 4.1112 - mean_absolute_error: 0.0168 - val_loss: 0.0039 - val_mean_absolute_percentage_error: 3.0930 - val_mean_absolute_error: 0.0459\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00468 to 0.00391, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 4/500\n",
      "2076/2076 [==============================] - 1s 530us/step - loss: 5.2723e-04 - mean_absolute_percentage_error: 4.0043 - mean_absolute_error: 0.0164 - val_loss: 0.0044 - val_mean_absolute_percentage_error: 3.2776 - val_mean_absolute_error: 0.0490\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00391\n",
      "Epoch 5/500\n",
      "2076/2076 [==============================] - 1s 491us/step - loss: 6.7704e-04 - mean_absolute_percentage_error: 4.5331 - mean_absolute_error: 0.0190 - val_loss: 0.0088 - val_mean_absolute_percentage_error: 4.5781 - val_mean_absolute_error: 0.0746\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00391\n",
      "Epoch 6/500\n",
      "2076/2076 [==============================] - 1s 493us/step - loss: 5.2304e-04 - mean_absolute_percentage_error: 3.9712 - mean_absolute_error: 0.0165 - val_loss: 0.0032 - val_mean_absolute_percentage_error: 2.7960 - val_mean_absolute_error: 0.0417\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00391 to 0.00323, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 7/500\n",
      "2076/2076 [==============================] - 1s 536us/step - loss: 3.8616e-04 - mean_absolute_percentage_error: 3.4286 - mean_absolute_error: 0.0140 - val_loss: 0.0029 - val_mean_absolute_percentage_error: 2.6439 - val_mean_absolute_error: 0.0394\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00323 to 0.00292, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 8/500\n",
      "2076/2076 [==============================] - 1s 503us/step - loss: 3.8130e-04 - mean_absolute_percentage_error: 3.3913 - mean_absolute_error: 0.0139 - val_loss: 0.0047 - val_mean_absolute_percentage_error: 3.2890 - val_mean_absolute_error: 0.0520\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00292\n",
      "Epoch 9/500\n",
      "2076/2076 [==============================] - 1s 498us/step - loss: 4.3415e-04 - mean_absolute_percentage_error: 3.5465 - mean_absolute_error: 0.0149 - val_loss: 0.0056 - val_mean_absolute_percentage_error: 3.7195 - val_mean_absolute_error: 0.0576\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00292\n",
      "Epoch 10/500\n",
      "2076/2076 [==============================] - 1s 519us/step - loss: 3.8460e-04 - mean_absolute_percentage_error: 3.3374 - mean_absolute_error: 0.0140 - val_loss: 0.0041 - val_mean_absolute_percentage_error: 3.2733 - val_mean_absolute_error: 0.0487\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00292\n",
      "Epoch 11/500\n",
      "2076/2076 [==============================] - 1s 534us/step - loss: 3.9449e-04 - mean_absolute_percentage_error: 3.5064 - mean_absolute_error: 0.0145 - val_loss: 0.0025 - val_mean_absolute_percentage_error: 2.4719 - val_mean_absolute_error: 0.0366\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00292 to 0.00251, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 12/500\n",
      "2076/2076 [==============================] - 1s 578us/step - loss: 5.6069e-04 - mean_absolute_percentage_error: 4.0570 - mean_absolute_error: 0.0173 - val_loss: 0.0048 - val_mean_absolute_percentage_error: 3.2666 - val_mean_absolute_error: 0.0531\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00251\n",
      "Epoch 13/500\n",
      "2076/2076 [==============================] - 1s 573us/step - loss: 3.5245e-04 - mean_absolute_percentage_error: 3.2965 - mean_absolute_error: 0.0137 - val_loss: 0.0048 - val_mean_absolute_percentage_error: 3.4684 - val_mean_absolute_error: 0.0539\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00251\n",
      "Epoch 14/500\n",
      "2076/2076 [==============================] - 1s 648us/step - loss: 3.4004e-04 - mean_absolute_percentage_error: 3.1417 - mean_absolute_error: 0.0132 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.5520 - val_mean_absolute_error: 0.0381\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00251\n",
      "Epoch 15/500\n",
      "2076/2076 [==============================] - 1s 573us/step - loss: 3.6182e-04 - mean_absolute_percentage_error: 3.2924 - mean_absolute_error: 0.0139 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2674 - val_mean_absolute_error: 0.0333\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00251 to 0.00213, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 16/500\n",
      "2076/2076 [==============================] - 1s 582us/step - loss: 3.4549e-04 - mean_absolute_percentage_error: 3.2968 - mean_absolute_error: 0.0135 - val_loss: 0.0037 - val_mean_absolute_percentage_error: 2.8653 - val_mean_absolute_error: 0.0459\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00213\n",
      "Epoch 17/500\n",
      "2076/2076 [==============================] - 1s 510us/step - loss: 3.4365e-04 - mean_absolute_percentage_error: 3.3123 - mean_absolute_error: 0.0137 - val_loss: 0.0023 - val_mean_absolute_percentage_error: 2.2847 - val_mean_absolute_error: 0.0346\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00213\n",
      "Epoch 18/500\n",
      "2076/2076 [==============================] - 1s 504us/step - loss: 4.2856e-04 - mean_absolute_percentage_error: 3.4463 - mean_absolute_error: 0.0146 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2521 - val_mean_absolute_error: 0.0336\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00213\n",
      "Epoch 19/500\n",
      "2076/2076 [==============================] - 1s 475us/step - loss: 5.0130e-04 - mean_absolute_percentage_error: 3.8060 - mean_absolute_error: 0.0160 - val_loss: 0.0090 - val_mean_absolute_percentage_error: 4.7018 - val_mean_absolute_error: 0.0782\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00213\n",
      "Epoch 20/500\n",
      "2076/2076 [==============================] - 1s 491us/step - loss: 4.3916e-04 - mean_absolute_percentage_error: 3.6599 - mean_absolute_error: 0.0151 - val_loss: 0.0068 - val_mean_absolute_percentage_error: 3.8900 - val_mean_absolute_error: 0.0653\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00213\n",
      "Epoch 21/500\n",
      "2076/2076 [==============================] - 1s 472us/step - loss: 3.4990e-04 - mean_absolute_percentage_error: 3.3585 - mean_absolute_error: 0.0136 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2326 - val_mean_absolute_error: 0.0333\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00213 to 0.00213, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 22/500\n",
      "2076/2076 [==============================] - 1s 463us/step - loss: 4.1060e-04 - mean_absolute_percentage_error: 3.5108 - mean_absolute_error: 0.0145 - val_loss: 0.0100 - val_mean_absolute_percentage_error: 4.9010 - val_mean_absolute_error: 0.0813\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00213\n",
      "Epoch 23/500\n",
      "2076/2076 [==============================] - 1s 487us/step - loss: 4.2169e-04 - mean_absolute_percentage_error: 3.5357 - mean_absolute_error: 0.0148 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.1992 - val_mean_absolute_error: 0.0330\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00213 to 0.00210, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 24/500\n",
      "2076/2076 [==============================] - 1s 450us/step - loss: 3.4503e-04 - mean_absolute_percentage_error: 3.2080 - mean_absolute_error: 0.0133 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.1895 - val_mean_absolute_error: 0.0331\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00210\n",
      "Epoch 25/500\n",
      "2076/2076 [==============================] - 1s 445us/step - loss: 3.1589e-04 - mean_absolute_percentage_error: 3.1116 - mean_absolute_error: 0.0128 - val_loss: 0.0042 - val_mean_absolute_percentage_error: 3.1685 - val_mean_absolute_error: 0.0497\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00210\n",
      "Epoch 26/500\n",
      "2076/2076 [==============================] - 1s 452us/step - loss: 3.3736e-04 - mean_absolute_percentage_error: 3.2232 - mean_absolute_error: 0.0133 - val_loss: 0.0034 - val_mean_absolute_percentage_error: 2.8513 - val_mean_absolute_error: 0.0446\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00210\n",
      "Epoch 27/500\n",
      "2076/2076 [==============================] - 1s 426us/step - loss: 3.0177e-04 - mean_absolute_percentage_error: 3.0828 - mean_absolute_error: 0.0126 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2138 - val_mean_absolute_error: 0.0333\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00210\n",
      "Epoch 28/500\n",
      "2076/2076 [==============================] - 1s 443us/step - loss: 5.6948e-04 - mean_absolute_percentage_error: 4.0759 - mean_absolute_error: 0.0172 - val_loss: 0.0057 - val_mean_absolute_percentage_error: 3.5651 - val_mean_absolute_error: 0.0590\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00210\n",
      "Epoch 29/500\n",
      "2076/2076 [==============================] - 1s 450us/step - loss: 4.8073e-04 - mean_absolute_percentage_error: 3.8093 - mean_absolute_error: 0.0158 - val_loss: 0.0046 - val_mean_absolute_percentage_error: 3.2047 - val_mean_absolute_error: 0.0523\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00210\n",
      "Epoch 30/500\n",
      "2076/2076 [==============================] - 1s 451us/step - loss: 4.2527e-04 - mean_absolute_percentage_error: 3.5022 - mean_absolute_error: 0.0148 - val_loss: 0.0058 - val_mean_absolute_percentage_error: 3.6075 - val_mean_absolute_error: 0.0589\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00210\n",
      "Epoch 31/500\n",
      "2076/2076 [==============================] - 1s 439us/step - loss: 2.9678e-04 - mean_absolute_percentage_error: 3.1229 - mean_absolute_error: 0.0125 - val_loss: 0.0054 - val_mean_absolute_percentage_error: 3.5931 - val_mean_absolute_error: 0.0572\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00210\n",
      "Epoch 32/500\n",
      "2076/2076 [==============================] - 1s 426us/step - loss: 2.7487e-04 - mean_absolute_percentage_error: 2.8637 - mean_absolute_error: 0.0119 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2086 - val_mean_absolute_error: 0.0329\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00210 to 0.00208, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 33/500\n",
      "2076/2076 [==============================] - 1s 425us/step - loss: 2.7857e-04 - mean_absolute_percentage_error: 2.9514 - mean_absolute_error: 0.0120 - val_loss: 0.0080 - val_mean_absolute_percentage_error: 4.3993 - val_mean_absolute_error: 0.0719\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00208\n",
      "Epoch 34/500\n",
      "2076/2076 [==============================] - 1s 441us/step - loss: 5.1385e-04 - mean_absolute_percentage_error: 3.7928 - mean_absolute_error: 0.0163 - val_loss: 0.0024 - val_mean_absolute_percentage_error: 2.3325 - val_mean_absolute_error: 0.0359\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00208\n",
      "Epoch 35/500\n",
      "2076/2076 [==============================] - 1s 448us/step - loss: 3.2361e-04 - mean_absolute_percentage_error: 3.1415 - mean_absolute_error: 0.0128 - val_loss: 0.0034 - val_mean_absolute_percentage_error: 2.8076 - val_mean_absolute_error: 0.0441\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00208\n",
      "Epoch 36/500\n",
      "2076/2076 [==============================] - 1s 443us/step - loss: 2.8220e-04 - mean_absolute_percentage_error: 3.0702 - mean_absolute_error: 0.0121 - val_loss: 0.0028 - val_mean_absolute_percentage_error: 2.5630 - val_mean_absolute_error: 0.0394\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00208\n",
      "Epoch 37/500\n",
      "2076/2076 [==============================] - 1s 426us/step - loss: 3.6333e-04 - mean_absolute_percentage_error: 3.3233 - mean_absolute_error: 0.0138 - val_loss: 0.0051 - val_mean_absolute_percentage_error: 3.4605 - val_mean_absolute_error: 0.0562\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00208\n",
      "Epoch 38/500\n",
      "2076/2076 [==============================] - 1s 443us/step - loss: 3.3679e-04 - mean_absolute_percentage_error: 3.1457 - mean_absolute_error: 0.0130 - val_loss: 0.0057 - val_mean_absolute_percentage_error: 3.6864 - val_mean_absolute_error: 0.0600\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00208\n",
      "Epoch 39/500\n",
      "2076/2076 [==============================] - 1s 440us/step - loss: 3.4234e-04 - mean_absolute_percentage_error: 3.2639 - mean_absolute_error: 0.0133 - val_loss: 0.0050 - val_mean_absolute_percentage_error: 3.3533 - val_mean_absolute_error: 0.0540\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00208\n",
      "Epoch 40/500\n",
      "2076/2076 [==============================] - 1s 450us/step - loss: 3.4997e-04 - mean_absolute_percentage_error: 3.2532 - mean_absolute_error: 0.0135 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.1822 - val_mean_absolute_error: 0.0327\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00208 to 0.00206, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 41/500\n",
      "2076/2076 [==============================] - 1s 446us/step - loss: 3.2928e-04 - mean_absolute_percentage_error: 3.2072 - mean_absolute_error: 0.0130 - val_loss: 0.0020 - val_mean_absolute_percentage_error: 2.1758 - val_mean_absolute_error: 0.0325\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00206 to 0.00203, saving model to weights/TATAMOTORS.NS.weights.best.hdf5\n",
      "Epoch 42/500\n",
      "2076/2076 [==============================] - 1s 445us/step - loss: 2.6177e-04 - mean_absolute_percentage_error: 2.8611 - mean_absolute_error: 0.0116 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2103 - val_mean_absolute_error: 0.0333\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00203\n",
      "Epoch 43/500\n",
      "2076/2076 [==============================] - 1s 451us/step - loss: 3.3106e-04 - mean_absolute_percentage_error: 3.1506 - mean_absolute_error: 0.0130 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2708 - val_mean_absolute_error: 0.0345\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00203\n",
      "Epoch 44/500\n",
      "2076/2076 [==============================] - 1s 453us/step - loss: 5.6366e-04 - mean_absolute_percentage_error: 3.8796 - mean_absolute_error: 0.0165 - val_loss: 0.0171 - val_mean_absolute_percentage_error: 6.9763 - val_mean_absolute_error: 0.1128\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00203\n",
      "Epoch 45/500\n",
      "2076/2076 [==============================] - 1s 454us/step - loss: 5.3705e-04 - mean_absolute_percentage_error: 3.9340 - mean_absolute_error: 0.0169 - val_loss: 0.0049 - val_mean_absolute_percentage_error: 3.3310 - val_mean_absolute_error: 0.0542\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00203\n",
      "Epoch 46/500\n",
      "2076/2076 [==============================] - 1s 446us/step - loss: 3.9237e-04 - mean_absolute_percentage_error: 3.4798 - mean_absolute_error: 0.0144 - val_loss: 0.0033 - val_mean_absolute_percentage_error: 2.7792 - val_mean_absolute_error: 0.0435\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00203\n",
      "Epoch 47/500\n",
      "2076/2076 [==============================] - 1s 447us/step - loss: 2.9608e-04 - mean_absolute_percentage_error: 3.0801 - mean_absolute_error: 0.0124 - val_loss: 0.0023 - val_mean_absolute_percentage_error: 2.3087 - val_mean_absolute_error: 0.0353\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00203\n",
      "Epoch 48/500\n",
      "2076/2076 [==============================] - 1s 448us/step - loss: 2.7952e-04 - mean_absolute_percentage_error: 2.9419 - mean_absolute_error: 0.0120 - val_loss: 0.0026 - val_mean_absolute_percentage_error: 2.4818 - val_mean_absolute_error: 0.0376\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00203\n",
      "Epoch 49/500\n",
      "2076/2076 [==============================] - 1s 447us/step - loss: 3.4236e-04 - mean_absolute_percentage_error: 3.1695 - mean_absolute_error: 0.0132 - val_loss: 0.0027 - val_mean_absolute_percentage_error: 2.5717 - val_mean_absolute_error: 0.0387\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00203\n",
      "Epoch 50/500\n",
      "2076/2076 [==============================] - 1s 435us/step - loss: 2.9786e-04 - mean_absolute_percentage_error: 3.0470 - mean_absolute_error: 0.0123 - val_loss: 0.0033 - val_mean_absolute_percentage_error: 2.6445 - val_mean_absolute_error: 0.0420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00203\n",
      "Epoch 51/500\n",
      "2076/2076 [==============================] - 1s 444us/step - loss: 3.2867e-04 - mean_absolute_percentage_error: 3.1933 - mean_absolute_error: 0.0131 - val_loss: 0.0047 - val_mean_absolute_percentage_error: 3.2733 - val_mean_absolute_error: 0.0532\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00203\n",
      "Epoch 52/500\n",
      "2076/2076 [==============================] - 1s 433us/step - loss: 3.0831e-04 - mean_absolute_percentage_error: 3.0740 - mean_absolute_error: 0.0126 - val_loss: 0.0049 - val_mean_absolute_percentage_error: 3.3990 - val_mean_absolute_error: 0.0543\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00203\n",
      "Epoch 53/500\n",
      "2076/2076 [==============================] - 1s 448us/step - loss: 3.4057e-04 - mean_absolute_percentage_error: 3.2426 - mean_absolute_error: 0.0134 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2000 - val_mean_absolute_error: 0.0332\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00203\n",
      "Epoch 54/500\n",
      "2076/2076 [==============================] - 1s 443us/step - loss: 3.1389e-04 - mean_absolute_percentage_error: 3.2387 - mean_absolute_error: 0.0130 - val_loss: 0.0031 - val_mean_absolute_percentage_error: 2.6472 - val_mean_absolute_error: 0.0417\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00203\n",
      "Epoch 55/500\n",
      "2076/2076 [==============================] - 1s 432us/step - loss: 3.6386e-04 - mean_absolute_percentage_error: 3.2736 - mean_absolute_error: 0.0137 - val_loss: 0.0025 - val_mean_absolute_percentage_error: 2.3927 - val_mean_absolute_error: 0.0370\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00203\n",
      "Epoch 56/500\n",
      "2076/2076 [==============================] - 1s 446us/step - loss: 2.7550e-04 - mean_absolute_percentage_error: 2.9514 - mean_absolute_error: 0.0119 - val_loss: 0.0022 - val_mean_absolute_percentage_error: 2.2721 - val_mean_absolute_error: 0.0345\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00203\n",
      "Epoch 57/500\n",
      "2076/2076 [==============================] - 1s 430us/step - loss: 2.5998e-04 - mean_absolute_percentage_error: 2.9975 - mean_absolute_error: 0.0118 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2109 - val_mean_absolute_error: 0.0331\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00203\n",
      "Epoch 58/500\n",
      "2076/2076 [==============================] - 1s 451us/step - loss: 3.0730e-04 - mean_absolute_percentage_error: 3.0877 - mean_absolute_error: 0.0126 - val_loss: 0.0047 - val_mean_absolute_percentage_error: 3.3690 - val_mean_absolute_error: 0.0539\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00203\n",
      "Epoch 59/500\n",
      "2076/2076 [==============================] - 1s 444us/step - loss: 3.7977e-04 - mean_absolute_percentage_error: 3.4628 - mean_absolute_error: 0.0143 - val_loss: 0.0051 - val_mean_absolute_percentage_error: 3.4525 - val_mean_absolute_error: 0.0559\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00203\n",
      "Epoch 60/500\n",
      "2076/2076 [==============================] - 1s 452us/step - loss: 3.7664e-04 - mean_absolute_percentage_error: 3.3567 - mean_absolute_error: 0.0140 - val_loss: 0.0021 - val_mean_absolute_percentage_error: 2.2114 - val_mean_absolute_error: 0.0332\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00203\n",
      "Epoch 61/500\n",
      "2076/2076 [==============================] - 1s 430us/step - loss: 3.8841e-04 - mean_absolute_percentage_error: 3.3178 - mean_absolute_error: 0.0139 - val_loss: 0.0031 - val_mean_absolute_percentage_error: 2.6464 - val_mean_absolute_error: 0.0416\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130db25f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039/1039 [==============================] - 0s 93us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0094185136865889, 1.6834844949729633, 0.07332274798909785]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "from models.KNNModel import KNNModel\n",
    "\n",
    "model = KNNModel(ticker, num_of_days, normalization_factor=10)\n",
    "\n",
    "y_pred = model.predict(model.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(model.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.10838384],\n",
       "       [3.10838384],\n",
       "       [3.10969697],\n",
       "       ...,\n",
       "       [3.10838384],\n",
       "       [3.10838384],\n",
       "       [3.10838384]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
